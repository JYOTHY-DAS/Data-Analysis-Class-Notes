# Day3: 
## Historical development of AI 
Presented by Mr. Sajil CK
### Enigma- II: A machine developed by Germany in WW II for data encryption. Modern computing started from the attempts to decrypt these messages. Mr. Allen Turing was key figure in braking these encrypted messages and now he is known as the father of modern computer.

### The idea of neural networks originated from the concept of neural pathway development in humans. Neuroplasticity is the adapting ability of brain which we can use in training neural networks.

### In 1958 Frank Rosenblatt developed the Mark I Perceptron machine  which is based on Perceptron algorithm developed by Warren McCulloch and Walter Pitts. The algorithm imitated the way neurons in the brain process information, and that enabled a system to learn from examples. We can say that it is a single layer neural network algorithm.

<p align="center">
  <img width="686" height="420" src="https://github.com/JYOTHY-DAS/Images/blob/main/S.jpg">
</p>

### A neuron will produce an output only if it receives a sufficient number of input signals, collectively at the dendrites. But if the amount of input signal is insufficient, the neuron will not produce an output. Perceptron mimicked the same method. It sums up the weighted inputs and if the result is above  a threshold value, it produces an output.


### 'Perceptrons: An Introduction to Computational Geometry' is a book written by Marvin Minsky and Seymour Papert and published in 1969. The crux of this book is a number of mathematical proofs which acknowledge some of the perceptron's strengths while also showing major limitations. The book proved that Perceptron machine won't be able to learn XOR function. After the publication of this book researchers started to question  the ability of 'Perceptron' to learn. The question raised was, if it cannot learn basic functions like XOR how can it learn complex tasks like walking, talking, writing etc.

<p align="center">
  <img width="1094" height="446" src="https://github.com/JYOTHY-DAS/Images/blob/main/p.png">
</p>

###  The limitations of Perceptron led to a loss of confidence in the potential of neural networks. Governments and industries became less willing to invest in AI research. This period is called as AI winter (mid-1970s to the early 1980s).

### Geoffrey Everest Hinton(Godfather of AI): His contributions to the development of multiple neuron models have been instrumental in advancing the field of AI.
### He started a free course in [Coursera](https://www.youtube.com/playlist?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9) on neural networks and machine learning. He received Nobel prize for physics in 2024.

## GPUs: The Powerhouses of Neural Network Training
### Why GPUs for Neural Networks?

### Graphics Processing Units (GPUs), originally designed for rendering graphics, have become indispensable tools for training neural networks. This is due to their:

- ### ***Massive parallel processing power*** which makes them ideal for the matrix operations involved in neural network training.
- ### ***High memory bandwidth***: High Memory Bandwidth refers to the rate at which data can be transferred between the memory and the processor. It's essentially the speed at which data can be read from or written to memory. GPUs have large amounts of high-bandwidth memory, allowing them to quickly access and process the large datasets required for training complex models.
- ### ***Specialized hardware accelerators***: Modern GPUs often include specialized hardware accelerators, such as Tensor Cores, that are specifically designed to accelerate deep learning operations.
### Popular GPU Choices for Neural Network Training: 
- #### NVIDIA GeForce RTX Series 
- #### NVIDIA Tesla Series
- #### AMD Radeon RX Series

## DARPA(Defense Advanced Research Projects Agency) grand challenge
### Fully autonomous vehicles have been an international pursuit for many years. The DARPA Grand Challenge is a prize competition for American autonomous vehicles, funded by the DARPA, the most prominent research organization of the United States Department of Defense. The Grand Challenge was the first long distance competition for driverless cars in the world. In the first DARPA challenge(2004) none of the robot vehicles finished the route. 

### The second DARPA Grand Challenge, 2005 played a pivotal role in accelerating the development of autonomous vehicle technology and, consequently, the broader field of AI. Here's why it was so significant:   

- ### 1. Catalyzed Technological Innovation:

### Pushed the Boundaries: The challenging off-road course forced teams to develop innovative solutions for perception, navigation, and control systems.   
### Accelerated Research: The competition spurred intense research and development efforts, leading to significant advancements in sensor technologies, computer vision, and machine learning algorithms.   

- ### 2. Fostered Collaboration and Knowledge Sharing:   

### Diverse Teams: The event brought together researchers from academia, industry, and government, fostering collaboration and knowledge exchange.   
### Open-Source Culture: Many teams shared their code and data, accelerating the development of autonomous vehicle technology for the entire community.

- ### 3. Laid the Foundation for Future Advancements:

### Technological Building Blocks: The technologies developed for the Grand Challenge, such as LiDAR, radar, and computer vision algorithms, became foundational for self-driving cars and other autonomous systems.   
### Inspiring the Next Generation: The event inspired young engineers and scientists to pursue careers in robotics and AI, ensuring a pipeline of talent for future innovation.

- ### 4. Demonstrated the Potential of AI:

### Public Awareness: The Grand Challenge showcased the potential of AI to solve complex real-world problems.   
### Investor interest: The success of the event attracted significant investment into autonomous vehicle technology, fueling further research and development.  

### In essence, the DARPA Grand Challenge 2005 served as a catalyst for the rapid advancement of autonomous vehicle technology and AI as a whole.


## AGI(Artificial general intelligence)
### Artificial General Intelligence (AGI) refers to the hypothetical intelligence of a machine that possesses the ability to understand or learn any intellectual task that a human being can. It's a type of artificial intelligence (AI) that aims to mimic the cognitive abilities of the human brain.   

### Key characteristics of AGI:

- ### Generalization ability: AGI can transfer knowledge and skills learned in one domain to another, enabling it to adapt to new and unseen situations effectively.   
- ### Learning and adaptation: AGI can learn from new information and experiences, making it capable of continuous improvement and evolution.
- ### Reasoning and problem-solving: AGI can think critically, analyze information, and solve complex problems.
- ### Self-awareness and consciousness: While not a strict requirement, some definitions of AGI include the ability to have subjective experiences and self-awareness.
- ### Current state of AGI:

- ### While significant progress has been made in AI, true AGI remains a theoretical concept. Current AI systems are specialized in specific tasks, such as image recognition or natural language processing. Creating an AI that can perform any intellectual task at a human level is a complex and challenging goal.
- ### Turing Test: The Turing test, originally called the imitation game by Alan Turing in 1949, is a test of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human.

## AI, Machine Learning, and Deep Learning: A Hierarchical Relationship
### Artificial Intelligence (AI)
- #### Broadest definition: AI is the science of making intelligent machines, especially intelligent computer programs.   
- #### Goal: To create intelligent agents that can reason, learn, and make decisions.   
### Machine Learning (ML)
- ####   Subset of AI: ML is a subset of AI that focuses on algorithms that allow computers to learn from data without being explicitly programmed.   
- #### Goal: To develop algorithms that can learn patterns from data and make predictions or decisions.   
### Deep Learning (DL)
- #### Subset of ML: DL is a subset of ML that uses artificial neural networks with multiple layers to learn complex patterns from large amounts of data.   
- #### Goal: To mimic the structure and function of the human brain to solve complex problems.
### Data Science: 
- #### Uses application of both machine learning and deep learning to extract insights from data.

<p align="center">
  <img width="420" height="420" src="https://github.com/JYOTHY-DAS/Images/blob/main/Hierarchy.png">
</p>

## Correlation vs. Causation: A Key Distinction
### Correlation and causation are two concepts often confused, yet they have distinct meanings.   

### **Correlation**
### Definition: Correlation refers to a statistical measure that indicates a relationship between two variables. It quantifies the strength and direction of this relationship.   
### Key point: Correlation does NOT imply causation.   
#### Example: Ice cream sales and drowning deaths are often positively correlated. However, eating ice cream doesn't cause drowning. A third variable, like warmer weather, influences both.   

### **Causation**
### Definition: Causation implies a cause-and-effect relationship between two variables. One variable directly influences the other.   
### Key point: Causation requires a deeper understanding of the underlying mechanisms and often involves controlled experiments.   
### Example: Smoking causes lung cancer. This is a causal relationship, supported by extensive scientific research.   

### Why is it important to distinguish between correlation and causation?

- ### Avoiding Misinterpretations: Incorrectly assuming causation can lead to flawed conclusions and poor decision-making.   
- ### Identifying True Relationships: Understanding the causal relationships between variables is crucial for effective problem-solving and scientific research.
- ### Making Informed Decisions: By recognizing the difference, we can make evidence-based decisions and avoid falling for false correlations.
- ### As the complexity of models and datasets increases, we often rely more on pattern recognition, it's important to remember that causal inference remains a valuable goal in many applications. 
### The choice between pattern matching and causal inference depends on the specific problem, the available data, and the desired outcome.
   
## Major Branches of Machine Learning

<p align="center">
  <img width="595" height="374" src="https://github.com/JYOTHY-DAS/Images/blob/main/Branches.png">
</p>

### Machine Learning is a vast field with various branches, each with its own unique approach to learning from data. Here are the primary branches:

### **Supervised Learning**
### In supervised learning, algorithms are trained on a labeled dataset. This means that each data point is associated with a correct output. The goal is to learn a mapping function that can predict the output for new, unseen data.

- ### Classification:
### Categorizes data into discrete classes.
### Examples: Email spam detection, image classification (cat vs. dog), disease diagnosis.
- ### Regression:
### Predicts a continuous numerical value.
### Examples: Predicting house prices, stock prices, or temperature.

### **Unsupervised Learning**
### Unlike supervised learning, unsupervised learning deals with unlabeled data. The goal is to discover hidden patterns and structures within the data.   

- ### Clustering:
### Groups similar data points together.
#### Examples: Customer segmentation, document clustering, image segmentation.
- ### Association Rule Mining:
### Discovers relationships between items in a dataset.
#### Examples: Market basket analysis (e.g., people who buy bread also buy milk).
- ### Reinforcement Learning
### Reinforcement learning involves training an agent to make decisions in an environment to maximize a reward signal.
#### Examples: Game playing (e.g., [AlphaGo](https://www.youtube.com/watch?v=WXuK6gekU1Y)), robotics, autonomous vehicles.


## Dynamic Memory Allocation vs. Static/Pre-Allocation of Memory
### Dynamic Memory Allocation

- ### Definition: Allocating memory at runtime as needed.   
- ### Process: Programmer requests memory from the heap using - functions like malloc, calloc, or realloc.
- ### Advantages:
- #### Flexibility: Memory can be allocated as required, without a fixed size at compile time.   
- #### Efficiency: Unused memory can be deallocated, optimizing memory usage.   
### Disadvantages:
- #### Complexity: Requires careful memory management to avoid memory leaks and errors.
- #### Overhead: Function calls and memory management operations can introduce overhead.
### Use Cases:
- #### Data structures with variable sizes (e.g., linked lists, trees).
- #### Allocating memory for large data sets that may vary in size.
- #### Dynamically resizing arrays or other data structures.
## Pre-Allocation of Memory

### Definition: Allocating memory at compile time, before program execution.
### Process: The compiler reserves a fixed amount of memory for variables.
### Advantages:
- #### Simplicity: Easier to manage and less prone to errors.
- #### Efficiency: Direct access to memory without overhead.
### Disadvantages:
- #### Limited Flexibility: Memory size is fixed and cannot be changed during runtime.
- #### Potential Waste: If the actual memory requirement is less, allocated memory is wasted.
### Use Cases:
- #### Arrays with fixed sizes.
- #### Simple data structures with predictable memory usage.
## Choosing the Right Approach

### The choice between dynamic and pre-allocation depends on several factors:

### Memory Requirements: If the memory requirements are known beforehand and relatively fixed, pre-allocation is often more efficient.
### Flexibility: If the memory needs are dynamic and may change during runtime, dynamic allocation is necessary.   
### Performance: Dynamic allocation can introduce overhead, so it's important to balance flexibility with performance.
### Error Handling: Dynamic allocation requires careful memory management to avoid memory leaks and other issues.   
### In many modern programming languages, memory management is often handled automatically through garbage collection, reducing the need for manual memory allocation and deallocation. However, understanding the concepts of dynamic and pre-allocation is still essential for efficient programming and debugging.

### Python primarily uses dynamic memory allocation. Python has a built-in memory manager that handles memory allocation and deallocation automatically.


## Object-Oriented Programming (OOP)
### It is a programming paradigm that organizes software design around data, or objects, rather than functions and logic. 
### It's a powerful approach that promotes 
- ### code reusability
- ### modularity: Code is organized into reusable modules)
- ### maintainability: Code is easier to understand, modify, and debug.
- 
<p align="center">
  <img width="396" height="377" src="https://github.com/JYOTHY-DAS/Images/blob/main/Code.jpg">
</p>

### **In this example:**

### Car is a class (A blueprint or template for creating objects) that defines the properties (color, model, year) and behaviors (start, stop) of a car.
### car1 and car2 are objects of the Car class, each with its own specific attributes.
### We can access the attributes and methods of these objects using the dot notation.

## Python Libraries
- ### NumPy (Numerical Python): Python library for large multi-dimensional array processing. Require less time for numerical operations than normal python. (Eg: Image processign)

- ### Pandas: Built on top of NumPy, which provides support for multi-dimensional arrays. I will make tabular(.csv: Comma separated values) operations faster and easier (Eg: Finding Max, Min, filter, sort etc.)

- ### Scikit learn: Machine Learning library (Not neural network method, only classical methods) in python

## Data science life cycle

Reference: https://analyticstraininghub.com/life-cycle-of-data-science/
 




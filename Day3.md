# Day3: Today's class was about the historical development of AI. The class was presented by Mr. Sajil CK
### First he introduced us to a machine developed by Germany in WW II for data encryption named as ENigma- II. Mr.Sajil said that modern computing started from the attempts to decrypt these messages. Mr. Allen Turing was key figure in braking these encrypted messages and now he is known as the father of modern computer.

### Then we discussed about the concept of neural networks. He said that the idea of neural networks originated from the concept of neural pathway development in humans. Neuroplasticity is the adapting ability of brain which we can use in training neural networks.

### In 1958 Frank Rosenblatt developed the Mark I Perceptron machine  which is based on Perceptron algorithm developed by Warren McCulloch and Walter Pitts. The algorithm imitated the way neurons in the brain process information, and that enabled a system to learn from examples. We can say that it is a single layer neural network algorithm.
(include S.jpg here)

### A neuron will produce an output only if it receives a sufficient number of input signals, collectively at the dendrites. But if the amount of input signal is insufficient, the neuron will not produce an output. Perceptron mimicked the same method. It sums up the weighted inputs and if the result is above  a threshold value, it produces an output.


### 'Perceptrons: An Introduction to Computational Geometry' is a book written by Marvin Minsky and Seymour Papert and published in 1969. The crux of this book is a number of mathematical proofs which acknowledge some of the perceptron's strengths while also showing major limitations. The book proved that Perceptron machine won't be able to learn XOR function. After the publication of this book researchers started to question  the ability of 'Perceptron' to learn. The question raised was, if it cannot learn basic functions like XOR how can it learn complex activities like walking, talking, writing etc.

(include p.png here)

###  The limitations of Perceptron led to a loss of confidence in the potential of neural networks. Governments and industries became less willing to invest in AI research. This period is called as AI winter(mid-1970s to the early 1980s).

### Geoffrey Everest Hinton(Godfather of AI): His contributions to the development of multiple neuron models have been instrumental in advancing the field of AI.
He started a free course in [Coursera](https://www.youtube.com/playlist?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9) on neural networks and machine learning. He received Nobel prize for physics in 2024.

## GPUs: The Powerhouses of Neural Network Training
### Why GPUs for Neural Networks?

- ### Graphics Processing Units (GPUs), originally designed for rendering graphics, have become indispensable tools for training neural networks. This is due to their:

- ### Massive Parallel Processing Power: GPUs are designed to handle many tasks simultaneously, making them ideal for the matrix operations involved in neural network training.
- ### High Memory Bandwidth: GPUs have large amounts of high-bandwidth memory, allowing them to quickly access and process the large datasets required for training complex models.
- ### Specialized Hardware Accelerators: Modern GPUs often include specialized hardware accelerators, such as Tensor Cores, that are specifically designed to accelerate deep learning operations.
- ### Popular GPU Choices for Neural Network Training: 1)NVIDIA GeForce RTX Series 2) NVIDIA Tesla Series 3) AMD Radeon RX Series

### DARPA(Defense Advanced Research Projects Agency) grand challenge: Fully autonomous vehicles have been an international pursuit for many years. The DARPA Grand Challenge is a prize competition for American autonomous vehicles, funded by the DARPA, the most prominent research organization of the United States Department of Defense. The Grand Challenge was the first long distance competition for driverless cars in the world. In the first DARPA challenge(2004) none of the robot vehicles finished the route. 

### The second DARPA Grand Challenge, 2005 played a pivotal role in accelerating the development of autonomous vehicle technology and, consequently, the broader field of AI. Here's why it was so significant:   

- ### 1. Catalyzed Technological Innovation:

### Pushed the Boundaries: The challenging off-road course forced teams to develop innovative solutions for perception, navigation, and control systems.   
### Accelerated Research: The competition spurred intense research and development efforts, leading to significant advancements in sensor technologies, computer vision, and machine learning algorithms.   

- ### 2. Fostered Collaboration and Knowledge Sharing:   

### Diverse Teams: The event brought together researchers from academia, industry, and government, fostering collaboration and knowledge exchange.   
### Open-Source Culture: Many teams shared their code and data, accelerating the development of autonomous vehicle technology for the entire community.

- ### 3. Laid the Foundation for Future Advancements:

### Technological Building Blocks: The technologies developed for the Grand Challenge, such as LiDAR, radar, and computer vision algorithms, became foundational for self-driving cars and other autonomous systems.   
### Inspiring the Next Generation: The event inspired young engineers and scientists to pursue careers in robotics and AI, ensuring a pipeline of talent for future innovation.

- ### 4. Demonstrated the Potential of AI:

### Public Awareness: The Grand Challenge showcased the potential of AI to solve complex real-world problems.   
### Investor Interest: The success of the event attracted significant investment into autonomous vehicle technology, fueling further research and development.   
### In essence, the DARPA Grand Challenge 2005 served as a catalyst for the rapid advancement of autonomous vehicle technology and AI as a whole.

### It demonstrated the power of competition, collaboration, and technological innovation to drive significant breakthroughs.








